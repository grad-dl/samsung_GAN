{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CycleGAN: Unpaired Image-to-Image Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Some setup before we begin...\n",
    "\n",
    "In this folder, please run the following commands to create empty folders for logging:\n",
    "\n",
    "```\n",
    "mkdir checkpoints_cyclegan\n",
    "mkdir samples_cyclegan\n",
    "```\n",
    "\n",
    "Also, make sure the unzipped dataset folder ```summer2winter_yosemite``` is in the same folder as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Visualize the Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Some matplotlib settings\n",
    "plt.rcParams[\"figure.figsize\"] = (16, 10)\n",
    "plt.rcParams[\"axes.grid\"] = False\n",
    "plt.rcParams[\"xtick.major.bottom\"] = False\n",
    "plt.rcParams[\"ytick.major.left\"] = False\n",
    "\n",
    "# Device Settings\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# Some training settings:\n",
    "batch_size = 16\n",
    "num_workers = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset can be downloaded from the following [Link](https://s3.amazonaws.com/video.udacity-data.com/topher/2018/November/5be66e78_summer2winter-yosemite/summer2winter-yosemite.zip).\n",
    "\n",
    "Move the ```summer2winter_yosemite``` folder into the git repo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define function to generate train and test dataloaders\n",
    "\n",
    "The following function returns train and test dataloaders, which we will use to visualize the data and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders(image_type, image_dir='summer2winter_yosemite', transform=None, batch_size=16, num_workers=0):\n",
    "    \n",
    "    # get training and test directories\n",
    "    image_type_test = \"test_{}\".format(image_type)\n",
    "    train_path = os.path.join(image_dir, image_type)\n",
    "    test_path = os.path.join(image_dir, image_type_test)\n",
    "\n",
    "    # define datasets using ImageFolder\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform)\n",
    "\n",
    "    # create and return DataLoaders\n",
    "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call the functions to get two sets of dataloaders. \n",
    "\n",
    "But first, we must define a image transformation function to transform the images from PIL format to Tensor format.\n",
    "\n",
    "We also normalize the image using standard mean and variances\n",
    "\n",
    "transforms.Compose() takes a list of transformations and calls them in sequence. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 256\n",
    "transformations = transforms.Compose([transforms.Resize(image_size), transforms.ToTensor(), \n",
    "                                      transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])])\n",
    "\n",
    "summer_dataloader, test_summer_dataloader = get_data_loaders(image_type='summer', transform=transformations, \n",
    "                                                            batch_size=batch_size, num_workers=num_workers)\n",
    "winter_dataloader, test_winter_dataloader = get_data_loaders(image_type='winter', transform=transformations,\n",
    "                                                            batch_size=batch_size, num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset created by ```datasets.ImageFolder``` return two values: \n",
    "\n",
    "1. Image\n",
    "2. corresponding label.\n",
    "\n",
    "When we wrap a dataloader around the dataset, this becomes a batch of images, and the corresponding labels. \n",
    "\n",
    "In our case, we are not interested in the labels (they are all 0). Thus, to visualize the data, we only save the first term of the returned values. To iterate just one batch of the dataset, we wrap the dataloader as an iterable and call one iteration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the data \n",
    "\n",
    "We define an inverse transform that changes a tensor back to a PIL Image. First, we \"unnormalize\" the tensor by taking the inverse of the normalization function from above. Then, we can call ```transforms.ToPILImage()``` which will convert the tensor to a PIL Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pil_img_from_tensor(img_tensor):\n",
    "    inverse_transform = transforms.Compose([transforms.Normalize(mean=[-1, -1, -1], std=[1/0.5, 1/0.5, 1/0.5]), \n",
    "                                            transforms.ToPILImage()])\n",
    "    return inverse_transform(img_tensor)\n",
    "\n",
    "def show_img(pil_img):\n",
    "    plt.imshow(pil_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_summer_imgs, _ = next(iter(summer_dataloader))\n",
    "batch_winter_imgs, _ = next(iter(winter_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(batch_summer_imgs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "show_img(get_pil_img_from_tensor(torchvision.utils.make_grid(batch_winter_imgs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Model \n",
    "\n",
    "A CycleGAN is composed of two discriminators and two generators.\n",
    "\n",
    "### Discriminators\n",
    "The discriminators, $D_X$ and $D_Y$, in this CycleGAN are convolutional neural networks that see an image and attempt to classify it as real or fake. In this case, real is indicated by an output close to 1 and fake as close to 0. The discriminators have the following architecture:\n",
    "\n",
    "<img src='imgs/discriminator_layers.png' width=80% />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A function that creates a general convolution layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# helper conv function. Note that DEFAULT stride = 2\n",
    "def conv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    conv_layer = nn.Conv2d(in_channels=in_channels, out_channels=out_channels, \n",
    "                           kernel_size=kernel_size, stride=stride, padding=padding, bias=False)\n",
    "    \n",
    "    layers.append(conv_layer)\n",
    "\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Discriminator\n",
    "\n",
    "Let's create the discriminator!\n",
    "\n",
    "> **Exercise**: Create a discriminator model using the `conv` function above. Refer to the image above for what the network should look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # Define all convolutional layers\n",
    "        # Should accept an RGB image as input and output a single value\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define feed forward\n",
    "        \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Discriminator\n",
    "\n",
    "with torch.no_grad():\n",
    "    D_X = Discriminator()\n",
    "    rand_input = torch.randn(3, 3, 128, 128)\n",
    "    out = D_X(rand_input)\n",
    "    assert(out.size() == (3, 1, 8, 8))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generator\n",
    "\n",
    "<img src='imgs/cyclegan_generator_ex.png' width=90% />\n",
    "\n",
    "\n",
    "### Residual Block\n",
    "\n",
    "<img src='imgs/resnet_block.png' width=40%/>\n",
    "\n",
    "Convolution output size formula:\n",
    "```\n",
    "output_size = 1 + (input_size - kernel_size + 2*padding) / stride\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# residual block class\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, conv_dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        # conv_dim = number of inputs\n",
    "        \n",
    "        # define two convolutional layers + batch normalization that will act as our residual function, F(x)\n",
    "        # layers should have the same shape input as output; I suggest a kernel_size of 3\n",
    "        \n",
    "        self.conv_layer1 = ?\n",
    "        \n",
    "        self.conv_layer2 = ?\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Fill out feed forward. Use F.relu() for relu\n",
    "        out = ?\n",
    "        out = ?\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconv Layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def deconv(in_channels, out_channels, kernel_size, stride=2, padding=1, batch_norm=True):\n",
    "    \"\"\"Creates a transpose convolutional layer, with optional batch normalization.\n",
    "    \"\"\"\n",
    "    layers = []\n",
    "    # append transpose conv layer\n",
    "    layers.append(nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding, bias=False))\n",
    "    # optional batch norm layer\n",
    "    if batch_norm:\n",
    "        layers.append(nn.BatchNorm2d(out_channels))\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Generator\n",
    "\n",
    "Use:\n",
    "* 3 conv layers,\n",
    "* Resnet layers (given)\n",
    "* 3 transpose conv layers\n",
    "\n",
    "Output shape of transpose conv can be calculated by the following formula:\n",
    "\n",
    "```\n",
    "output_size = strides * (input_size-1) + kernel_size - 2*padding\n",
    "```\n",
    "\n",
    "select the correct input dims, output dims and kernel size! (Default stride = 2, default padding = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CycleGenerator(nn.Module):\n",
    "    \n",
    "    def __init__(self, conv_dim=64, n_res_blocks=6):\n",
    "        super(CycleGenerator, self).__init__()\n",
    "\n",
    "        # Encoder \n",
    "\n",
    "        # Resnet part. Hint: add Residual blocks to a list, then use nn.Sequential(*res_layers)\n",
    "        # This is useful for taking a variable number of res_blocks\n",
    "\n",
    "        # 3. Decoder\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Given an image x, returns a transformed image.\"\"\"\n",
    "        # define feedforward behavior, applying activations as necessary. Make sure to finish off with tanh activation\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Generator\n",
    "with torch.no_grad():\n",
    "    G_XtoY = CycleGenerator()\n",
    "    rand_input = torch.randn(3, 3, 128, 128)\n",
    "    out = G_XtoY(rand_input)\n",
    "    assert(out.size() == rand_input.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Creator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(g_conv_dim=64, d_conv_dim=64, n_res_blocks=6, device='cuda'):\n",
    "    \n",
    "    # Instantiate generators\n",
    "    G_XtoY = ?\n",
    "    G_YtoX = ?\n",
    "    \n",
    "    # Instantiate discriminators\n",
    "    D_X = ?\n",
    "    D_Y = ?\n",
    "\n",
    "    # Cast to appropriate device. \n",
    "    G_XtoY.to(device)\n",
    "    G_YtoX.to(device)\n",
    "    D_X.to(device)\n",
    "    D_Y.to(device)\n",
    "    print('Models loaded on {}'.format(device))\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_XtoY, G_YtoX, D_X, D_Y = create_model(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At this point, you can open a terminal on the server and run ```nvidia-smi```. On the first GPU, you will see that some memory has been allocated for your model (~1GB). \n",
    "\n",
    "If you want to skip training and just visualize samples, skip to Section 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss functions \n",
    "\n",
    "* Use `torch.mean()` for mean across inputs.\n",
    "* Use `torch.abs()` to get absolute value of a tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_mse_loss(D_out):\n",
    "    # How close is the produced output from being REAL?\n",
    "    return ?\n",
    "\n",
    "def fake_mse_loss(D_out):\n",
    "    # How close is the produced output from being FAKE?\n",
    "    return ?\n",
    "\n",
    "def cycle_consistency_loss(real_im, reconstructed_im, lambda_weight):\n",
    "    # Reconstruction loss\n",
    "    reconstr_loss = ?\n",
    "    return lambda_weight*reconstr_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Optimizers \n",
    "\n",
    "- We will use Adam with a initial learning rate of ```lr=0.0002```. \n",
    "\n",
    "- In PyTorch, we pass the model's parameters into the optimizer. The model's parameters can be obtained by calling ```model.parameters()```. \n",
    "\n",
    "- When we want to optimize the parameters of two models with one optimizer, we can simply concatenate the list of the two models' parameters and pass that into the optimizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Hyperparameters for Adam optimizer\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "beta2 = 0.999\n",
    "\n",
    "d_x_optimizer = optim.Adam(params=D_X.parameters(), lr=lr, betas=[beta1, beta2])\n",
    "d_y_optimizer = optim.Adam(params=D_Y.parameters(), lr=lr, betas=[beta1, beta2])\n",
    "\n",
    "# Create a single optimizer for both generators\n",
    "generator_params = ?\n",
    "g_optimizer = optim.Adam(params=generator_params, lr=lr, betas=[beta1, beta2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Loop \n",
    "\n",
    "Skip this part if you want to skip directly to visualizing the results with a pretrained network.\n",
    "\n",
    "Training takes around **80 minutes** on a single **NVIDIA 2080Ti** GPU (CUDA 10.2) for 6000 iterations. It will use up around **7GB** of VRAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from helpers import save_samples, checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(dataloader_X, dataloader_Y, test_dataloader_X, test_dataloader_Y, \n",
    "                  n_epochs=1000):\n",
    "    \n",
    "    since = time.time()\n",
    "    print_every=50\n",
    "    sample_every=100\n",
    "    \n",
    "    losses = []      # keep track of losses over time\n",
    "\n",
    "    \n",
    "    test_iter_X = iter(test_dataloader_X)\n",
    "    test_iter_Y = iter(test_dataloader_Y)\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = test_iter_X.next()[0]\n",
    "    fixed_Y = test_iter_Y.next()[0]\n",
    "\n",
    "    # batches per epoch\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "    batches_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for epoch in range(1, n_epochs+1):\n",
    "\n",
    "        # Reset iterators for each epoch\n",
    "        if epoch % batches_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, _ = iter_X.next()\n",
    "        images_Y, _ = iter_Y.next()\n",
    "        \n",
    "        # move images to device\n",
    "        images_X = images_X.to(device)\n",
    "        images_Y = images_Y.to(device)\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "        \"\"\"D_X \"\"\" \n",
    "        d_x_optimizer.zero_grad()\n",
    "\n",
    "        # Train with real X images\n",
    "        \n",
    "        out_x = ?\n",
    "        D_X_real_loss = ?\n",
    "        \n",
    "        # Train with fake X images\n",
    "        fake_X = ?\n",
    "        out_x = ?\n",
    "        D_X_fake_loss = ?\n",
    "        \n",
    "        d_x_loss = D_X_real_loss + D_X_fake_loss\n",
    "        d_x_loss.backward()\n",
    "        d_x_optimizer.step()\n",
    "        \n",
    "        \"\"\"D_Y \"\"\" \n",
    "        d_y_optimizer.zero_grad()\n",
    "        \n",
    "        # Train with real Y images\n",
    "        out_y = ?\n",
    "        D_Y_real_loss = ?\n",
    "        \n",
    "        # Train with fake Y images\n",
    "        fake_Y = ?\n",
    "        out_y = ?\n",
    "        D_Y_fake_loss = ?\n",
    "\n",
    "        d_y_loss = D_Y_real_loss + D_Y_fake_loss\n",
    "        d_y_loss.backward()\n",
    "        d_y_optimizer.step()\n",
    "\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "        \"\"\"G : G_XtoY + G_YtoX \n",
    "        For reconstructed loss, use a lambda weight of 10\n",
    "        \"\"\" \n",
    "        \n",
    "\n",
    "        g_optimizer.zero_grad()\n",
    "        \n",
    "        fake_X = ?\n",
    "        out_x = ?\n",
    "        g_YtoX_loss = ?\n",
    "\n",
    "        fake_Y = ?\n",
    "        out_y = ?\n",
    "        g_XtoY_loss = ?\n",
    "\n",
    "        reconstructed_X = ?\n",
    "        reconstructed_x_loss = ?\n",
    "        \n",
    "        reconstructed_Y = ?\n",
    "        reconstructed_y_loss = ?\n",
    "\n",
    "        g_total_loss = g_YtoX_loss + g_XtoY_loss + reconstructed_y_loss + reconstructed_x_loss\n",
    "        g_total_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print\n",
    "        if epoch % print_every == 0:\n",
    "            # append real and fake discriminator losses and the generator loss\n",
    "            losses.append((d_x_loss.item(), d_y_loss.item(), g_total_loss.item()))\n",
    "            time_elapsed = time.time() - since\n",
    "            print('Epoch [{:5d}/{:5d}] | d_X_loss: {:6.4f} | d_Y_loss: {:6.4f} | g_total_loss: {:6.4f} time : {:.0f}m {:.0f}s'.format(\n",
    "                    epoch, n_epochs, d_x_loss.item(), d_y_loss.item(), g_total_loss.item(),time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "        \n",
    "        if epoch % sample_every == 0:\n",
    "            G_YtoX.eval() # set generators to eval mode for sample generation\n",
    "            G_XtoY.eval()\n",
    "            save_samples(epoch, fixed_Y, fixed_X, G_YtoX, G_XtoY, batch_size=16, sample_dir='samples_cyclegan')\n",
    "            G_YtoX.train()\n",
    "            G_XtoY.train()\n",
    "\n",
    "        checkpoint_every=1\n",
    "        if epoch % checkpoint_every == 0:\n",
    "            checkpoint(epoch, G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_epochs = 6000 # keep this small when testing if a model first works\n",
    "\n",
    "losses = training_loop(summer_dataloader, winter_dataloader, test_summer_dataloader, test_winter_dataloader, n_epochs=n_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Training Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "losses = np.array(losses)\n",
    "plt.plot(losses.T[0], label='Discriminator, X', alpha=0.5)\n",
    "plt.plot(losses.T[1], label='Discriminator, Y', alpha=0.5)\n",
    "plt.plot(losses.T[2], label='Generators', alpha=0.5)\n",
    "plt.title(\"Training Losses\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualize the Results "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (Optional) Load Pretrained Models\n",
    "\n",
    "If we lack the time and resources to fully train the model, we can load a pretrained model and visualize the results. Use the ```save_samples``` function as in the training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import load_checkpoints\n",
    "load_checkpoints(G_XtoY, G_YtoX, D_X, D_Y, checkpoint_dir='checkpoints_cyclegan_pretrained')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.image as mpimg\n",
    "\n",
    "# helper visualization code\n",
    "def view_samples(iteration, sample_dir='samples_cyclegan'):\n",
    "    \n",
    "    # samples are named by iteration\n",
    "    path_XtoY = os.path.join(sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    path_YtoX = os.path.join(sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    \n",
    "    # read in those samples\n",
    "    try: \n",
    "        x2y = mpimg.imread(path_XtoY)\n",
    "        y2x = mpimg.imread(path_YtoX)\n",
    "    except:\n",
    "        print('Invalid number of iterations.')\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(18,20), nrows=2, ncols=1, sharey=True, sharex=True)\n",
    "    ax1.imshow(x2y)\n",
    "    ax1.set_title('X to Y')\n",
    "    ax2.imshow(y2x)\n",
    "    ax2.set_title('Y to X')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view samples at iteration 6000\n",
    "view_samples(6000, 'samples_cyclegan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view_samples(1000, 'samples_cyclegan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
